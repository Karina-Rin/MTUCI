{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Лабораторная работа 3**\n",
    "\n",
    "***\n",
    "__ПОСИИ__\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортирование необходимых модулей\n",
    "import numpy as np # NumPy используется для работы с массивами и матрицами\n",
    "import tensorflow as tf # TensorFlow используется для создания и обучения нейронных сетей\n",
    "mnist = tf.keras.datasets.mnist\n",
    "to_categorical = tf.keras.utils.to_categorical\n",
    "Sequential = tf.keras.models.Sequential\n",
    "Dense = tf.keras.layers.Dense\n",
    "\n",
    "# Загрузка данных MNIST, который содержит изображения рукописных цифр\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Преобразование изображений в одномерные массивы . Также преобразуем метки классов в формат one-hot (бинарный, 0 и 1)\n",
    "x_train = x_train.reshape(x_train.shape[0], -1) / 255.0 # Нормализуем значения пикселей\n",
    "x_test = x_test.reshape(x_test.shape[0], -1) / 255.0 # Нормализуем значения пикселей\n",
    "y_train = to_categorical(y_train) # Преобразуем метки классов в бинарный формат\n",
    "y_test = to_categorical(y_test) # Преобразуем метки классов в бинарный формат"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция `to_categorical` преобразует метки классов (например, 0, 1, 2 для трех классов) в бинарный формат `one-hot`. Это означает, что для каждого класса создается отдельный столбец, где значение 1 указывает на принадлежность к классу, а 0 — на отсутствие.\n",
    "\n",
    "Например, если у нас три класса, метка 1 будет представлена как [0, 1, 0].\n",
    "\n",
    "Первая строка или столбец может быть [1, 0, 0], если это соответствует классу 0. Однако, [0, 0, 1] будет представлять класс 2. В зависимости от того, какой класс мы кодируем, мы можем получить разные представления.\n",
    "Порядок классов в `one-hot` кодировании зависит от того, как мы их задали в нашем наборе данных. Если классы идут в порядке 0, 1, 2, то представления будут именно такими."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 974us/step - accuracy: 0.7549 - loss: 0.8422 - val_accuracy: 0.9228 - val_loss: 0.2753\n",
      "Epoch 2/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 928us/step - accuracy: 0.9088 - loss: 0.3192 - val_accuracy: 0.9278 - val_loss: 0.2504\n",
      "Epoch 3/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 927us/step - accuracy: 0.9176 - loss: 0.2873 - val_accuracy: 0.9340 - val_loss: 0.2347\n",
      "Epoch 4/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 922us/step - accuracy: 0.9218 - loss: 0.2745 - val_accuracy: 0.9365 - val_loss: 0.2280\n",
      "Epoch 5/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 991us/step - accuracy: 0.9239 - loss: 0.2720 - val_accuracy: 0.9382 - val_loss: 0.2231\n",
      "Epoch 6/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9278 - loss: 0.2614 - val_accuracy: 0.9367 - val_loss: 0.2267\n",
      "Epoch 7/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9293 - loss: 0.2535 - val_accuracy: 0.9387 - val_loss: 0.2222\n",
      "Epoch 8/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9293 - loss: 0.2489 - val_accuracy: 0.9403 - val_loss: 0.2225\n",
      "Epoch 9/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 928us/step - accuracy: 0.9305 - loss: 0.2521 - val_accuracy: 0.9388 - val_loss: 0.2229\n",
      "Epoch 10/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 931us/step - accuracy: 0.9320 - loss: 0.2441 - val_accuracy: 0.9398 - val_loss: 0.2222\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x239aa9ab950>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Архитектура сети 1:\n",
    "model = Sequential() # Создаем последовательную модель с двумя полносвязными слоями, которая представляет собой линейную последовательность слоев\n",
    "model.add(Dense(10, input_dim=784, activation='relu')) # Первый слой имеет 10 нейронов, размерность входных данных 784 с активацией ReLU\n",
    "model.add(Dense(10, activation='softmax')) # Второй слой имеет 10 нейронов с активацией softmax для многоклассовой классификации\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', # Компилируем модель с функцией потерь categorical_crossentropy и оптимизатором Adam\n",
    "metrics=['accuracy']) # Указываем, что хотим отслеживать метрику accuracy (точность) во время обучения\n",
    "\n",
    "# Обучение\n",
    "model.fit(x_train, y_train, epochs=10, validation_split=0.1) # Используем 10 эпох и 10% данных для валидации, чтобы отслеживать производительность модели на невидимых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 923us/step - accuracy: 0.9163 - loss: 0.2945\n",
      "[0.2629081904888153, 0.9248999953269958]\n"
     ]
    }
   ],
   "source": [
    "# Анализ результата 1 на проверочных и тестовых данных\n",
    "test_acc = model.evaluate(x_test, y_test) # Используем функцию model.evaluate() для оценки производительности обученной модели на заданном наборе данных\n",
    "print(test_acc) # Выводим точность"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В результате получаем информацию о точности (`accuracy`) и потере (`loss`) модели на тестовых данных. Модель оценивает 313 батчей (пакетов) данных. Каждый батч содержит определенное количество изображений из тестового набора.\n",
    "\n",
    "- Точность (`accuracy`): 0.9163\n",
    "\n",
    "Модель правильно классифицировала 91.63% тестовых примеров. Это довольно высокий уровень точности, что говорит о хорошей работе модели на тестовых данных.\n",
    "\n",
    "- Потеря (`loss`): 0.2945\n",
    "\n",
    "Функция потерь измеряет, насколько хорошо модель предсказывает результаты. Чем ниже это значение, тем лучше модель справляется с задачей. В данном случае, потеря в 0.2945 также указывает на то, что модель работает достаточно эффективно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8427 - loss: 0.5664 - val_accuracy: 0.9550 - val_loss: 0.1677\n",
      "Epoch 2/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9460 - loss: 0.1867 - val_accuracy: 0.9632 - val_loss: 0.1292\n",
      "Epoch 3/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9619 - loss: 0.1303 - val_accuracy: 0.9670 - val_loss: 0.1089\n",
      "Epoch 4/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9719 - loss: 0.0982 - val_accuracy: 0.9707 - val_loss: 0.0995\n",
      "Epoch 5/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9755 - loss: 0.0823 - val_accuracy: 0.9710 - val_loss: 0.1011\n",
      "Epoch 6/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9790 - loss: 0.0706 - val_accuracy: 0.9728 - val_loss: 0.0913\n",
      "Epoch 7/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9809 - loss: 0.0609 - val_accuracy: 0.9755 - val_loss: 0.0933\n",
      "Epoch 8/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9833 - loss: 0.0550 - val_accuracy: 0.9733 - val_loss: 0.0958\n",
      "Epoch 9/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9855 - loss: 0.0498 - val_accuracy: 0.9742 - val_loss: 0.1035\n",
      "Epoch 10/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 995us/step - accuracy: 0.9871 - loss: 0.0426 - val_accuracy: 0.9753 - val_loss: 0.0975\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2399c59c450>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Архитектура сети 2:\n",
    "model2 = Sequential() # Создаем последовательную модель с двумя полносвязными слоями, которая представляет собой линейную последовательность слоев\n",
    "model2.add(Dense(50, input_dim=784, activation='relu')) # Увеличиваем количество нейронов в первом слое (10 -> 50), размерность входных данных 784 с активацией ReLU\n",
    "model2.add(Dense(10, activation='softmax')) # Второй слой имеет 10 нейронов с активацией softmax для многоклассовой классификации\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # Компилируем модель с функцией потерь categorical_crossentropy и оптимизатором Adam. Указываем, что хотим отслеживать метрику accuracy (точность) во время обучения\n",
    "model2.fit(x_train, y_train, epochs=10, validation_split=0.1) # Используем 10 эпох и 10% данных для валидации, чтобы отслеживать производительность модели на невидимых данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Увеличение числа нейронов может улучшить способность модели к обучению, позволяя ей захватывать более сложные паттерны."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 801us/step - accuracy: 0.9692 - loss: 0.1182\n",
      "[0.09958184510469437, 0.9733999967575073]\n"
     ]
    }
   ],
   "source": [
    "# Анализ результата 2 на проверочных и тестовых данных\n",
    "test_acc = model2.evaluate(x_test, y_test) # Используем функцию model.evaluate() для оценки производительности обученной модели на заданном наборе данных\n",
    "print(test_acc) # Выводим точность"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В результате получаем информацию о точности (`accuracy`) и потере (`loss`) второй модели на тестовых данных. Модель оценивает 313 батчей (пакетов) данных. Каждый батч содержит определенное количество изображений из тестового набора.\n",
    "\n",
    "- Точность (`accuracy`): 0.9692\n",
    "\n",
    "Модель правильно классифицировала 96.92% тестовых примеров. Это довольно высокий уровень точности, что говорит о хорошей работе модели на тестовых данных.\n",
    "\n",
    "- Потеря (`loss`): 0.1182\n",
    "\n",
    "Функция потерь измеряет, насколько хорошо модель предсказывает результаты. Чем ниже это значение, тем лучше модель справляется с задачей. В данном случае, потеря в 0.1182 также указывает на то, что модель работает достаточно эффективно.\n",
    "\n",
    "\n",
    "---\n",
    "Сравнивая результаты двух моделей, можно сделать следующие выводы:\n",
    "\n",
    "- Улучшение `точности`: вторая модель показывает значительно более высокую `точность` (96.92%) по сравнению с первой моделью (91.63%). Это указывает на то, что вторая модель более эффективна в классификации тестовых данных.\n",
    "- Снижение `потерь`: `потеря` также значительно снизилась с 0.2945 для первой модели до 0.1182 для второй модели, что подтверждает улучшение производительности.\n",
    "\n",
    "Таким образом, можно заключить, что вторая модель значительно улучшила точность и снизила потери по сравнению с первой моделью."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.8397 - loss: 0.5519 - val_accuracy: 0.9600 - val_loss: 0.1372\n",
      "Epoch 2/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9546 - loss: 0.1542 - val_accuracy: 0.9680 - val_loss: 0.1098\n",
      "Epoch 3/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9667 - loss: 0.1084 - val_accuracy: 0.9677 - val_loss: 0.1067\n",
      "Epoch 4/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9751 - loss: 0.0836 - val_accuracy: 0.9738 - val_loss: 0.0916\n",
      "Epoch 5/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9789 - loss: 0.0662 - val_accuracy: 0.9747 - val_loss: 0.0917\n",
      "Epoch 6/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9821 - loss: 0.0577 - val_accuracy: 0.9748 - val_loss: 0.0980\n",
      "Epoch 7/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9852 - loss: 0.0476 - val_accuracy: 0.9755 - val_loss: 0.0875\n",
      "Epoch 8/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9871 - loss: 0.0407 - val_accuracy: 0.9740 - val_loss: 0.1020\n",
      "Epoch 9/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9884 - loss: 0.0358 - val_accuracy: 0.9753 - val_loss: 0.0933\n",
      "Epoch 10/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9915 - loss: 0.0272 - val_accuracy: 0.9712 - val_loss: 0.1136\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x239a1b52710>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Архитектура сети 3:\n",
    "model3 = Sequential() # Создаем более сложную модель, которая представляет собой линейную последовательность слоев более сложную модель, с двумя скрытыми слоями по 50 нейронов\n",
    "model3.add(Dense(50, input_dim=784, activation='relu')) # Первый слой имеет 50 нейронов, размерность входных данных 784 с активацией ReLU\n",
    "model3.add(Dense(50, activation='relu')) # Второй полносвязный слой имеет 50 нейронов с активацией ReLU, выходы принимает из первого слоя как свои входные данные\n",
    "model3.add(Dense(10, activation='softmax')) # Выходной слой имеет 10 нейронов с активацией Softmax\n",
    "model3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])# Компилируем модель с функцией потерь categorical_crossentropy и оптимизатором Adam. Указываем, что хотим отслеживать метрику accuracy (точность) во время обучения\n",
    "model3.fit(x_train, y_train, epochs=10, validation_split=0.1) # Используем 10 эпох и 10% данных для валидации, чтобы отслеживать производительность модели на невидимых данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавляем полносвязный (плотный) 1-й слой Dense с 50 нейронами. Параметр `input_dim=784` указывает, что входные данные имеют размерность `784` (например, это может быть изображение 28x28 пикселей, развернутое в одномерный массив). Функция активации `relu` (Rectified Linear Unit) помогает модели обучаться быстрее и справляться с проблемой затухающего градиента.\n",
    "\n",
    "Далее - еще один полносвязный слой с 50 нейронами и также используем функцию активации `relu`. Этот слой будет принимать выходы из предыдущего слоя как свои входные данные.\n",
    "\n",
    "В выходном слое добавляем 10 нейронов, что соответствует количеству классов, которые мы хотим предсказать (например, цифры от 0 до 9). Функция активации `softmax` преобразует выходы в вероятности, которые суммируются до 1, что позволяет интерпретировать их как вероятности принадлежности к каждому классу.\n",
    "\n",
    "Далее компилируем модель, указывая функцию потерь `categorical_crossentropy`, которая подходит для многоклассовой классификации. Оптимизатор `adam` используется для обновления весов модели во время обучения.\n",
    "\n",
    "Указываем, что хотим отслеживать метрику `accuracy` (точность) во время обучения.\n",
    "\n",
    "Обучаем модель на тренировочных данных `x_train` и соответствующих метках `y_train`. Параметр `epochs=10` указывает, что мы хотим пройти через весь набор данных 10 раз. `validation_split=0.1` означает, что 10% данных будет использовано для валидации модели во время обучения, что позволяет отслеживать ее производительность на невидимых данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 891us/step - accuracy: 0.9606 - loss: 0.1456\n",
      "[0.12438387423753738, 0.9664000272750854]\n"
     ]
    }
   ],
   "source": [
    "# Анализ результата 3 на проверочных и тестовых данных.\n",
    "test_acc = model3.evaluate(x_test, y_test) # Используем функцию model.evaluate() для оценки производительности обученной модели на заданном наборе данных\n",
    "print(test_acc) # Выводим точность"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В результате получаем информацию о точности (`accuracy`) и потере (`loss`) третьей модели на тестовых данных. Модель оценивает 313 батчей (пакетов) данных. Каждый батч содержит определенное количество изображений из тестового набора.\n",
    "\n",
    "- Точность (`accuracy`): 0.9606\n",
    "\n",
    "Модель правильно классифицировала 96.06% тестовых примеров. Это довольно высокий уровень точности, что говорит о хорошей работе модели на тестовых данных.\n",
    "\n",
    "- Потеря (`loss`): 0.1456\n",
    "\n",
    "Функция потерь измеряет, насколько хорошо модель предсказывает результаты. Чем ниже это значение, тем лучше модель справляется с задачей. В данном случае, потеря в 0.1456 также указывает на то, что модель работает достаточно эффективно.\n",
    "\n",
    "\n",
    "---\n",
    "Сравнивая результаты трех моделей, можно сделать следующие выводы:\n",
    "\n",
    "- Улучшение `точности`: вторая модель демонстрирует наилучшие результаты с точностью 96.92%, что на 5.29% выше, чем у первой модели. Третья модель также показывает высокую точность, но немного уступает второй.\n",
    "- Снижение `потерь`: все модели показывают снижение потерь по сравнению с первой моделью, что также указывает на улучшение производительности.\n",
    "\n",
    "Таким образом, вторая модель является наиболее эффективной из трех, и ее использование может привести к лучшим результатам в задачах классификации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "__Сверточная нейронная сеть (CNN)__\n",
    "- это нейронная сеть, которая может «видеть» подмножество наших данных. С помощью нее можно обнаружить образ на изображениях лучше, чем при работе с персептроном."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортование необходимых модулей создания сверточной нейронной сети (CNN)\n",
    "Conv2D = tf.keras.layers.Conv2D\n",
    "MaxPooling2D = tf.keras.layers.MaxPooling2D\n",
    "Flatten = tf.keras.layers.Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка набора данных Fashion MNIST и подготовка его аналогично MNIST\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist # Импортируем набор данных Fashion MNIST из библиотеки TensorFlow\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data() # Загружаем данные из набора Fashion MNIST\n",
    "x_train = x_train[:,:,:,np.newaxis] / 255.0 # Изменяем форму массива x_train, добавляя новую ось (изменяя его размерность) и нормализуем значения пикселей, деля их на 255.0\n",
    "x_test = x_test[:,:,:,np.newaxis] / 255.0 # Изменяем формы массива x_test и нормализуем значения пикселей\n",
    "y_train = to_categorical(y_train) # Преобразуем метки классов y_train в формат one-hot (0, 1)\n",
    "y_test = to_categorical(y_test) # Преобразуем метки классов y_test в формат one-hot (0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `(x_train, y_train)` - обучающая выборка\n",
    "- `(x_test, y_test)` - тестовая выборка\n",
    "\n",
    "`np.newaxis`- это специальный объект в библиотеке NumPy, используется для добавления новой оси к массиву, что позволяет изменить его размерность. Это необходимо для корректной обработки данных в нейронной сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.8023 - loss: 0.5763 - val_accuracy: 0.8818 - val_loss: 0.3288\n",
      "Epoch 2/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.8891 - loss: 0.3165 - val_accuracy: 0.8957 - val_loss: 0.3050\n",
      "Epoch 3/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.9019 - loss: 0.2789 - val_accuracy: 0.8868 - val_loss: 0.3094\n",
      "Epoch 4/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9078 - loss: 0.2593 - val_accuracy: 0.8987 - val_loss: 0.2839\n",
      "Epoch 5/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.9134 - loss: 0.2404 - val_accuracy: 0.9032 - val_loss: 0.2678\n",
      "Epoch 6/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.9219 - loss: 0.2192 - val_accuracy: 0.8950 - val_loss: 0.3018\n",
      "Epoch 7/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.9241 - loss: 0.2166 - val_accuracy: 0.9047 - val_loss: 0.2719\n",
      "Epoch 8/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9299 - loss: 0.2010 - val_accuracy: 0.9027 - val_loss: 0.2797\n",
      "Epoch 9/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.9303 - loss: 0.1959 - val_accuracy: 0.9058 - val_loss: 0.2686\n",
      "Epoch 10/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.9338 - loss: 0.1849 - val_accuracy: 0.9062 - val_loss: 0.2712\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x239a1e8d210>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Архитектура сети 4:\n",
    "model4 = Sequential() # Создаем модель Sequential\n",
    "model4.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', input_shape=(28,28, 1))) # Добавляется сверточный слой Conv2D с 64 фильтрами, размером ядра 2x2, с использованием функции активации ReLU и с параметром padding='same', который сохраняет размерность входного изображения\n",
    "model4.add(MaxPooling2D(pool_size=2)) # Добавляем слой подвыборки MaxPooling2D с размером окна 2x2\n",
    "model4.add(Flatten()) # Добавляем слой Flatten, который преобразует многомерный выход из предыдущего слоя в одномерный вектор\n",
    "model4.add(Dense(10, activation='softmax')) # Добавляем полносвязный слой Dense с 10 нейронами и функцией активации softmax\n",
    "model4.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # Компилируем модель с использованием функции потерь categorical_crossentropy, оптимизатора adam и метрики accuracy\n",
    "model4.fit(x_train, y_train, epochs=10, validation_split=0.1) # Запускаем процесс обучения модели на обучающих данных x_train и y_train на протяжении 10 эпох, используя 10% данных для валидации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параметр `input_shape` указывает форму входных данных (28x28 пикселей с 1 каналом).\n",
    "\n",
    "Слой подвыборки уменьшает размерность выходных данных, что снижает вычислительные затраты и помогает избежать переобучения, сохраняя при этом важные признаки.\n",
    "\n",
    "Слой Flatten необходим для передачи данных в полносвязный слой, так как полносвязные слои ожидают одномерные входные данные.\n",
    "\n",
    "Полносвязный слой используется для классификации, где каждый нейрон соответствует одному классу. Функция активации `softmax` преобразует выходные значения в вероятности, что позволяет интерпретировать их как предсказания классов.\n",
    "\n",
    "Компиляция модели необходима для определения, как будет происходить обучение. `categorical_crossentropy` подходит для многоклассовой классификации, а `adam` — это эффективный оптимизатор, который адаптирует скорость обучения.\n",
    "\n",
    "Обучение модели позволяет ей адаптироваться к данным, а валидация помогает отслеживать производительность модели на невидимых данных, что важно для предотвращения переобучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9021 - loss: 0.2795\n",
      "[0.27876409888267517, 0.9009000062942505]\n"
     ]
    }
   ],
   "source": [
    "# Анализ результата 4 на проверочных и тестовых данных.\n",
    "test_acc = model4.evaluate(x_test, y_test) # Используем функцию model.evaluate() для оценки производительности обученной модели на заданном наборе данных\n",
    "print(test_acc) # Выводим точность"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В результате получаем информацию о точности (`accuracy`) и потере (`loss`) третьей модели на тестовых данных. Модель оценивает 313 батчей (пакетов) данных. Каждый батч содержит определенное количество изображений из тестового набора.\n",
    "\n",
    "- Точность (`accuracy`): 0.9021\n",
    "\n",
    "Модель правильно классифицировала 90.21% тестовых примеров. Это довольно высокий уровень точности, что говорит о хорошей работе модели на тестовых данных.\n",
    "\n",
    "- Потеря (`loss`): 0.2765\n",
    "\n",
    "Функция потерь измеряет, насколько хорошо модель предсказывает результаты. Чем ниже это значение, тем лучше модель справляется с задачей. В данном случае, потеря в 0.2765 также указывает на то, что модель работает достаточно эффективно.\n",
    "\n",
    "---\n",
    "Сравнивая результаты четырех моделей, можно сделать следующие выводы:\n",
    "\n",
    "- Улучшение `точности`: вторая модель демонстрирует наилучшие результаты с точностью 96.92%, что на 5.29% выше, чем у первой модели. Третья модель также показывает высокую точность (96.06%), но немного уступает второй. Четвертая модель, напротив, имеет наименьшую точность (90.21%), что указывает на необходимость доработки.\n",
    "- Снижение `потерь`: вторая модель также показывает наименьшие потери (0.1182), что свидетельствует о ее высокой эффективности. Третья модель имеет потери 0.1456, что также является хорошим результатом. Первая модель имеет потери 0.2945, а четвертая модель — 0.2795, что указывает на менее эффективное обучение по сравнению с другими моделями.\n",
    "\n",
    "Таким образом, вторая модель является наиболее эффективной, с высокой точностью и низкими потерями. Четвертая модель требует доработки, так как ее точность значительно ниже по сравнению с другими моделями. Для достижения лучших результатов в задачах классификации стоит рассмотреть возможность использования второй модели или улучшения четвертой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "##### **Проведем сравнение работы четырех архитектур. Какая из архитектур показывает лучший результат и почему? Как можно улучшить результаты каждой из архитектур?**\n",
    "\n",
    "###### Архитектура 1: Простая полносвязная сеть\n",
    "- Структура: 1 скрытый слой с 10 нейронами.\n",
    "- Результаты: Ожидается, что эта архитектура покажет низкую точность, так как она слишком проста для решения задачи классификации изображений.\n",
    "\n",
    "###### Архитектура 2: Увеличенный скрытый слой\n",
    "- Структура: 1 скрытый слой с 50 нейронами.\n",
    "- Результаты: Эта архитектура должна показать улучшение по сравнению с первой, так как большее количество нейронов позволяет лучше захватывать паттерны в данных.\n",
    "\n",
    "###### Архитектура 3: Глубокая полносвязная сеть\n",
    "- Структура: 2 скрытых слоя, каждый с 50 нейронами.\n",
    "- Результаты: Ожидается, что эта архитектура будет иметь наилучшие результаты среди полносвязных сетей, так как она может моделировать более сложные функции.\n",
    "\n",
    "###### Архитектура 4: Сверточная нейронная сеть (CNN)\n",
    "- Структура: Сверточный слой, слой подвыборки и полносвязный слой.\n",
    "- Результаты: CNN обычно показывает наилучшие результаты для задач классификации изображений, так как они могут эффективно извлекать пространственные и временные зависимости из изображений.\n",
    "\n",
    "***\n",
    "##### Сравнение результатов\n",
    "Наилучшие результаты: архитектура 4 (CNN) показывает наилучшие результаты по сравнению с другими архитектурами, так как она специально разработана для обработки изображений и может извлекать более сложные признаки.\n",
    "***\n",
    "\n",
    "##### Улучшение других архитектур:\n",
    "- Архитектура 1 и 2: Можно увеличить количество нейронов в скрытых слоях и добавить регуляризацию (например, Dropout), чтобы избежать переобучения.\n",
    "- Архитектура 3: Можно попробовать увеличить количество слоев или нейронов, а также использовать различные функции активации (например, Leaky ReLU).\n",
    "- Архитектура 4: Можно добавить больше сверточных слоев, использовать различные размеры ядер свертки и увеличить количество фильтров для улучшения извлечения признаков.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По вариантам (в соответствии с номером в списке) спроектируйте архитектуру, реализуйте алгоритм корректировки синаптических весов с помощью алгоритма обратного распространения ошибки, используя в качестве функции активации логистический сигмоид 𝑓(𝑛𝑒𝑡) = 1/(1+exp(-net))\n",
    "\n",
    "**Вариант 4**\n",
    "- Архитектура: 3-4-3\n",
    "- Скорость обучения: 0.4\n",
    "- Входной вектор: X={0.4; -0.7; 1.3}\n",
    "- Матрицы синапсов 1 и 2 слоя: Начальные значения весов взять произвольным образом из интервала [-0.3 0.3]\n",
    "- Эталонный выход: Y = {0.3; -0.5; 0.8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обученные веса между входным и скрытым слоями:\n",
      "[[ 0.18684483  0.26355246 -0.12372625 -0.15696435]\n",
      " [ 0.07021499  0.04153902  0.28196033 -0.11643026]\n",
      " [ 0.07206802  0.43060545 -0.07259561  0.37731315]]\n",
      "Обученные веса между скрытым и выходным слоями:\n",
      "[[ 0.01168556 -0.64172869  0.3624883 ]\n",
      " [-0.09305864 -0.96050232  0.49752545]\n",
      " [-0.16492517 -0.30156217  0.06746499]\n",
      " [-0.00866185 -0.79258816 -0.03695764]]\n",
      "Предсказанный выход после обучения:\n",
      "[0.47208287 0.17617597 0.62324994]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # Импортируем библиотеку NumPy для работы с массивами и математическими операциями\n",
    "import tensorflow as tf # Импортируем TensorFlow, который будет использоваться для создания и обучения нейронной сети\n",
    "Sequential = tf.keras.models.Sequential\n",
    "Dense = tf.keras.layers.Dense\n",
    "\n",
    "# Установка скорости обучения для обновления весов\n",
    "learning_rate = 0.3 # Определяем скорость обучения для обновления весов во время обучения\n",
    "\n",
    "# Входной вектор\n",
    "input_vector = np.array([0.4, -0.7, 1.3]) # Создаем одномерный массив с тремя элементами (функция создает массив NumPy из предоставленного списка или последовательности)\n",
    "\n",
    "# Эталонный выход\n",
    "target_output = np.array([0.3, -0.5, 0.8]) # Определяем целевой выход (функция создает массив NumPy из предоставленного списка или последовательности)\n",
    "\n",
    "# Инициализация весов случайными значениями из интервала [-0.3, 0.3]\n",
    "weights_input_hidden = np.random.uniform(-0.3, 0.3, (3, 4))   # Инициализируем веса между входным и скрытым слоями случайными значениями (функция генерирует массив случайных чисел, распределенных равномерно в заданном диапазоне)\n",
    "weights_hidden_output = np.random.uniform(-0.3, 0.3, (4, 3))  # Инициализируем веса между скрытым и выходным слоями случайными значениями (функция генерирует массив случайных чисел, распределенных равномерно в заданном диапазоне)\n",
    "\n",
    "# Функция активации: логистическая сигмоида используется для преобразования входных значений в диапазон (0, 1)\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Функция активации: логистическая сигмоида.\n",
    "    \n",
    "    Параметры:\n",
    "    x (numpy.ndarray): Входные данные для функции активации.\n",
    "    \n",
    "    Возвращает:\n",
    "    numpy.ndarray: Выходные данные после применения сигмоиды.\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x)) # Возвращаем значение сигмоиды для входных данных (функция вычисляет экспоненту для каждого элемента массива)\n",
    "\n",
    "# Производная функции активации необходима для обратного распространения ошибки при обновлении весов.\n",
    "def sigmoid_derivative(x): \n",
    "    \"\"\"\n",
    "    Производная функции активации сигмоиды.\n",
    "    \n",
    "    Параметры:\n",
    "    x (numpy.ndarray): Выходные данные функции активации.\n",
    "    \n",
    "    Возвращает:\n",
    "    numpy.ndarray: Производная функции активации.\n",
    "    \"\"\"\n",
    "    return x * (1 - x) # Возвращаем производную сигмоиды для вычисления градиентов\n",
    "\n",
    "# Обучение сети\n",
    "for epoch in range(25):  # Количество эпох\n",
    "    # Прямое распространение\n",
    "    hidden_layer_input = np.dot(input_vector, weights_input_hidden) # Вычисляем входные данные для скрытого слоя (функция выполняет матричное умножение)\n",
    "    hidden_layer_output = sigmoid(hidden_layer_input) # Применяем функцию активации к входным данным скрытого слоя\n",
    "    output_layer_input = np.dot(hidden_layer_output, weights_hidden_output) # Вычисляем входные данные для выходного слоя (функция выполняет матричное умножение)\n",
    "    output_layer_output = sigmoid(output_layer_input) # Применяем функцию активации к входным данным выходного слоя\n",
    "\n",
    "    # Ошибка\n",
    "    error = target_output - output_layer_output # Вычисляем ошибку между целевым выходом и предсказанным выходом\n",
    "\n",
    "    # Обратное распространение\n",
    "    d_predicted_output = error * output_layer_output * (1 - output_layer_output) # Вычисляем градиент для выходного слоя\n",
    "    error_hidden_layer = d_predicted_output.dot(weights_hidden_output.T) # Вычисляем ошибку для скрытого слоя\n",
    "    d_hidden_layer = error_hidden_layer * hidden_layer_output * (1 - hidden_layer_output) # Вычисляем градиент для скрытого слоя.\n",
    "\n",
    "    # Обновление весов\n",
    "    weights_hidden_output += hidden_layer_output.reshape(-1, 1).dot(d_predicted_output.reshape(1, -1)) * learning_rate # Обновляем веса между скрытым и выходным слоями\n",
    "    weights_input_hidden += input_vector.reshape(-1, 1).dot(d_hidden_layer.reshape(1, -1)) * learning_rate # Обновляем веса между входным и скрытым слоями\n",
    "\n",
    "# Результаты после обучения\n",
    "print(\"Обученные веса между входным и скрытым слоями:\")\n",
    "print(weights_input_hidden)\n",
    "print(\"Обученные веса между скрытым и выходным слоями:\")\n",
    "print(weights_hidden_output)\n",
    "\n",
    "# Проверка предсказания\n",
    "print(\"Предсказанный выход после обучения:\")\n",
    "print(output_layer_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Контрольные вопросы:\n",
    "**1. Понятие нейронной сети, архитектуры**\n",
    "\n",
    "Нейронная сеть — это вычислительная модель, вдохновленная структурой и функциями биологических нейронных сетей. Она состоит из узлов (нейронов), которые связаны между собой и могут обрабатывать информацию.\n",
    "\n",
    "Архитектура нейронной сети определяется количеством слоев (входной, скрытые и выходной) и количеством нейронов в каждом слое.\n",
    "\n",
    "**2. Обучение нейронной сети**\n",
    "\n",
    "Обучение нейронной сети — это процесс, в ходе которого сеть настраивает свои параметры (веса) на основе обучающей выборки. Это позволяет сети минимизировать ошибку предсказания и улучшать свою производительность на новых данных.\n",
    "\n",
    "**3. Основные определения: скорость обучения, эпоха, нейрон, обучающая выборка, тестовая выборка, вариационная выборка, функция активации.**\n",
    "\n",
    "- Скорость обучения — это параметр, который определяет, насколько сильно обновляются веса нейронной сети на каждом шаге обучения. Высокая скорость может привести к нестабильности, а низкая — к медленному обучению.\n",
    "- Эпоха — это один полный проход через всю обучающую выборку. Обычно обучение включает множество эпох.\n",
    "- Нейрон — это базовый элемент нейронной сети, который принимает входные данные, применяет к ним веса и функцию активации, чтобы выдать выходное значение.\n",
    "- Обучающая выборка — это набор данных, используемый для обучения модели.\n",
    "- Тестовая выборка — это набор данных, используемый для оценки производительности модели после обучения.\n",
    "- Вариационная выборка — это набор данных, который может использоваться для настройки гиперпараметров модели.\n",
    "- Функция активации — это функция, которая определяет выход нейрона на основе его входных данных. Она помогает сети обучаться сложным зависимостям.\n",
    "\n",
    "**4. Алгоритм обратного распространения ошибки**\n",
    "\n",
    "Алгоритм обратного распространения ошибки (backpropagation) — это метод, используемый для обучения нейронных сетей. Он включает в себя два этапа: прямое распространение, где входные данные проходят через сеть и генерируют выход, и обратное распространение, где вычисляется градиент ошибки и обновляются веса сети. Этот процесс повторяется до тех пор, пока ошибка не станет приемлемой.\n",
    "\n",
    "**5. Типы функций активации**\n",
    "\n",
    "Существует несколько типов функций активации, включая:\n",
    "- Сигмоидная функция — преобразует входные данные в диапазон от 0 до 1.\n",
    "- Гиперболический тангенс — преобразует входные данные в диапазон от -1 до 1.\n",
    "- ReLU (Rectified Linear Unit) — возвращает 0 для отрицательных входов и сам вход для положительных, что помогает избежать проблемы затухающего градиента.\n",
    "\n",
    "**6. Алгоритмы обучения**\n",
    "\n",
    "Существует несколько алгоритмов обучения нейронных сетей, включая:\n",
    "- Градиентный спуск — основной метод, который минимизирует функцию потерь, обновляя веса в направлении отрицательного градиента.\n",
    "- Стохастический градиентный спуск (SGD) — вариант градиентного спуска, который обновляет веса на основе одного примера из обучающей выборки.\n",
    "- Адаптивные методы (например, Adam, RMSprop) — алгоритмы, которые адаптируют скорость обучения для каждого веса на основе их градиентов."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
