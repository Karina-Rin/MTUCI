{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Лабораторная работа 3**\n",
    "\n",
    "***\n",
    "__ПОСИИ__\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортование необходимых модулей\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "to_categorical = tf.keras.utils.to_categorical\n",
    "Sequential = tf.keras.models.Sequential\n",
    "Dense = tf.keras.layers.Dense\n",
    "\n",
    "# Загрузка данных\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Преобразование данных\n",
    "x_train = x_train.reshape(x_train.shape[0], -1) / 255.0\n",
    "x_test = x_test.reshape(x_test.shape[0], -1) / 255.0\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 974us/step - accuracy: 0.7549 - loss: 0.8422 - val_accuracy: 0.9228 - val_loss: 0.2753\n",
      "Epoch 2/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 928us/step - accuracy: 0.9088 - loss: 0.3192 - val_accuracy: 0.9278 - val_loss: 0.2504\n",
      "Epoch 3/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 927us/step - accuracy: 0.9176 - loss: 0.2873 - val_accuracy: 0.9340 - val_loss: 0.2347\n",
      "Epoch 4/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 922us/step - accuracy: 0.9218 - loss: 0.2745 - val_accuracy: 0.9365 - val_loss: 0.2280\n",
      "Epoch 5/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 991us/step - accuracy: 0.9239 - loss: 0.2720 - val_accuracy: 0.9382 - val_loss: 0.2231\n",
      "Epoch 6/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9278 - loss: 0.2614 - val_accuracy: 0.9367 - val_loss: 0.2267\n",
      "Epoch 7/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9293 - loss: 0.2535 - val_accuracy: 0.9387 - val_loss: 0.2222\n",
      "Epoch 8/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9293 - loss: 0.2489 - val_accuracy: 0.9403 - val_loss: 0.2225\n",
      "Epoch 9/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 928us/step - accuracy: 0.9305 - loss: 0.2521 - val_accuracy: 0.9388 - val_loss: 0.2229\n",
      "Epoch 10/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 931us/step - accuracy: 0.9320 - loss: 0.2441 - val_accuracy: 0.9398 - val_loss: 0.2222\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x239aa9ab950>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Архитектура сети 1:\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=784, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
    "metrics=['accuracy'])\n",
    "\n",
    "# Обучение:\n",
    "model.fit(x_train, y_train, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 923us/step - accuracy: 0.9163 - loss: 0.2945\n",
      "[0.2629081904888153, 0.9248999953269958]\n"
     ]
    }
   ],
   "source": [
    "# Анализ результата 1 на проверочных и тестовых данных.\n",
    "test_acc = model.evaluate(x_test, y_test)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8427 - loss: 0.5664 - val_accuracy: 0.9550 - val_loss: 0.1677\n",
      "Epoch 2/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9460 - loss: 0.1867 - val_accuracy: 0.9632 - val_loss: 0.1292\n",
      "Epoch 3/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9619 - loss: 0.1303 - val_accuracy: 0.9670 - val_loss: 0.1089\n",
      "Epoch 4/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9719 - loss: 0.0982 - val_accuracy: 0.9707 - val_loss: 0.0995\n",
      "Epoch 5/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9755 - loss: 0.0823 - val_accuracy: 0.9710 - val_loss: 0.1011\n",
      "Epoch 6/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9790 - loss: 0.0706 - val_accuracy: 0.9728 - val_loss: 0.0913\n",
      "Epoch 7/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9809 - loss: 0.0609 - val_accuracy: 0.9755 - val_loss: 0.0933\n",
      "Epoch 8/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9833 - loss: 0.0550 - val_accuracy: 0.9733 - val_loss: 0.0958\n",
      "Epoch 9/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9855 - loss: 0.0498 - val_accuracy: 0.9742 - val_loss: 0.1035\n",
      "Epoch 10/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 995us/step - accuracy: 0.9871 - loss: 0.0426 - val_accuracy: 0.9753 - val_loss: 0.0975\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2399c59c450>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Архитектура сети 2:\n",
    "model2 = Sequential()\n",
    "model2.add(Dense( 50, input_dim=784, activation='relu'))\n",
    "model2.add(Dense(10, activation='softmax'))\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam',\n",
    "metrics=['accuracy'])\n",
    "model2.fit(x_train, y_train, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 801us/step - accuracy: 0.9692 - loss: 0.1182\n",
      "[0.09958184510469437, 0.9733999967575073]\n"
     ]
    }
   ],
   "source": [
    "# Анализ результата 2 на проверочных и тестовых данных.\n",
    "test_acc = model2.evaluate(x_test, y_test)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.8397 - loss: 0.5519 - val_accuracy: 0.9600 - val_loss: 0.1372\n",
      "Epoch 2/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9546 - loss: 0.1542 - val_accuracy: 0.9680 - val_loss: 0.1098\n",
      "Epoch 3/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9667 - loss: 0.1084 - val_accuracy: 0.9677 - val_loss: 0.1067\n",
      "Epoch 4/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9751 - loss: 0.0836 - val_accuracy: 0.9738 - val_loss: 0.0916\n",
      "Epoch 5/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9789 - loss: 0.0662 - val_accuracy: 0.9747 - val_loss: 0.0917\n",
      "Epoch 6/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9821 - loss: 0.0577 - val_accuracy: 0.9748 - val_loss: 0.0980\n",
      "Epoch 7/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9852 - loss: 0.0476 - val_accuracy: 0.9755 - val_loss: 0.0875\n",
      "Epoch 8/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9871 - loss: 0.0407 - val_accuracy: 0.9740 - val_loss: 0.1020\n",
      "Epoch 9/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9884 - loss: 0.0358 - val_accuracy: 0.9753 - val_loss: 0.0933\n",
      "Epoch 10/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9915 - loss: 0.0272 - val_accuracy: 0.9712 - val_loss: 0.1136\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x239a1b52710>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Архитектура сети 3:\n",
    "model3 = Sequential()\n",
    "model3.add(Dense(50, input_dim=784, activation='relu'))\n",
    "model3.add(Dense(50, activation='relu'))\n",
    "model3.add(Dense(10, activation='softmax'))\n",
    "model3.compile(loss='categorical_crossentropy', optimizer='adam',\n",
    "metrics=['accuracy'])\n",
    "model3.fit(x_train, y_train, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 891us/step - accuracy: 0.9606 - loss: 0.1456\n",
      "[0.12438387423753738, 0.9664000272750854]\n"
     ]
    }
   ],
   "source": [
    "# Анализ результата 3 на проверочных и тестовых данных.\n",
    "test_acc = model3.evaluate(x_test, y_test)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортование необходимых модулей\n",
    "Conv2D = tf.keras.layers.Conv2D\n",
    "MaxPooling2D = tf.keras.layers.MaxPooling2D\n",
    "Flatten = tf.keras.layers.Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "x_train = x_train[:,:,:,np.newaxis] / 255.0\n",
    "x_test = x_test[:,:,:,np.newaxis] / 255.0\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.8023 - loss: 0.5763 - val_accuracy: 0.8818 - val_loss: 0.3288\n",
      "Epoch 2/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.8891 - loss: 0.3165 - val_accuracy: 0.8957 - val_loss: 0.3050\n",
      "Epoch 3/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.9019 - loss: 0.2789 - val_accuracy: 0.8868 - val_loss: 0.3094\n",
      "Epoch 4/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9078 - loss: 0.2593 - val_accuracy: 0.8987 - val_loss: 0.2839\n",
      "Epoch 5/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.9134 - loss: 0.2404 - val_accuracy: 0.9032 - val_loss: 0.2678\n",
      "Epoch 6/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.9219 - loss: 0.2192 - val_accuracy: 0.8950 - val_loss: 0.3018\n",
      "Epoch 7/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.9241 - loss: 0.2166 - val_accuracy: 0.9047 - val_loss: 0.2719\n",
      "Epoch 8/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9299 - loss: 0.2010 - val_accuracy: 0.9027 - val_loss: 0.2797\n",
      "Epoch 9/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.9303 - loss: 0.1959 - val_accuracy: 0.9058 - val_loss: 0.2686\n",
      "Epoch 10/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.9338 - loss: 0.1849 - val_accuracy: 0.9062 - val_loss: 0.2712\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x239a1e8d210>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Архитектура сети 4:\n",
    "model4 = Sequential()\n",
    "model4.add(Conv2D(filters=64, kernel_size=2, padding='same',\n",
    "activation='relu', input_shape=(28,28, 1)))\n",
    "model4.add(MaxPooling2D(pool_size=2))\n",
    "model4.add(Flatten())\n",
    "model4.add(Dense(10, activation='softmax'))\n",
    "model4.compile(loss='categorical_crossentropy', optimizer='adam',\n",
    "metrics=['accuracy'])\n",
    "model4.fit(x_train, y_train, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9021 - loss: 0.2795\n",
      "[0.27876409888267517, 0.9009000062942505]\n"
     ]
    }
   ],
   "source": [
    "# Анализ результата 4 на проверочных и тестовых данных.\n",
    "test_acc = model4.evaluate(x_test, y_test)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "##### **Проведите сравнение работы четырех архитектур. Какая из архитектур показывает лучший результат и почему? Как можно улучшить результаты каждой из архитектур?**\n",
    "\n",
    "###### Архитектура 1: Простая полносвязная сеть\n",
    "- Структура: 1 скрытый слой с 10 нейронами.\n",
    "- Результаты: Ожидается, что эта архитектура покажет низкую точность, так как она слишком проста для решения задачи классификации изображений.\n",
    "\n",
    "###### Архитектура 2: Увеличенный скрытый слой\n",
    "- Структура: 1 скрытый слой с 50 нейронами.\n",
    "- Результаты: Эта архитектура должна показать улучшение по сравнению с первой, так как большее количество нейронов позволяет лучше захватывать паттерны в данных.\n",
    "\n",
    "###### Архитектура 3: Глубокая полносвязная сеть\n",
    "- Структура: 2 скрытых слоя, каждый с 50 нейронами.\n",
    "- Результаты: Ожидается, что эта архитектура будет иметь наилучшие результаты среди полносвязных сетей, так как она может моделировать более сложные функции.\n",
    "\n",
    "###### Архитектура 4: Сверточная нейронная сеть (CNN)\n",
    "- Структура: Сверточный слой, слой подвыборки и полносвязный слой.\n",
    "- Результаты: CNN обычно показывает наилучшие результаты для задач классификации изображений, так как они могут эффективно извлекать пространственные и временные зависимости из изображений.\n",
    "\n",
    "***\n",
    "##### Сравнение результатов\n",
    "Наилучшие результаты: архитектура 4 (CNN) показывает наилучшие результаты по сравнению с другими архитектурами, так как она специально разработана для обработки изображений и может извлекать более сложные признаки.\n",
    "***\n",
    "\n",
    "##### Улучшение других архитектур:\n",
    "- Архитектура 1 и 2: Можно увеличить количество нейронов в скрытых слоях и добавить регуляризацию (например, Dropout), чтобы избежать переобучения.\n",
    "- Архитектура 3: Можно попробовать увеличить количество слоев или нейронов, а также использовать различные функции активации (например, Leaky ReLU).\n",
    "- Архитектура 4: Можно добавить больше сверточных слоев, использовать различные размеры ядер свертки и увеличить количество фильтров для улучшения извлечения признаков.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По вариантам (в соответствии с номером в списке) спроектируйте архитектуру, реализуйте алгоритм корректировки синаптических весов с помощью алгоритма обратного распространения ошибки, используя в качестве функции активации логистический сигмоид 𝑓(𝑛𝑒𝑡) = 1/(1+exp(-net))\n",
    "\n",
    "**Вариант 4**\n",
    "- Архитектура: 3-4-3\n",
    "- Скорость обучения: 0.4\n",
    "- Входной вектор: X={0.4; -0.7; 1.3}\n",
    "- Матрицы синапсов 1 и 2 слоя: Начальные значения весов взять произвольным образом из интервала [-0.3 0.3]\n",
    "- Эталонный выход: Y = {0.3; -0.5; 0.8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обученные веса между входным и скрытым слоями:\n",
      "[[ 0.18684483  0.26355246 -0.12372625 -0.15696435]\n",
      " [ 0.07021499  0.04153902  0.28196033 -0.11643026]\n",
      " [ 0.07206802  0.43060545 -0.07259561  0.37731315]]\n",
      "Обученные веса между скрытым и выходным слоями:\n",
      "[[ 0.01168556 -0.64172869  0.3624883 ]\n",
      " [-0.09305864 -0.96050232  0.49752545]\n",
      " [-0.16492517 -0.30156217  0.06746499]\n",
      " [-0.00866185 -0.79258816 -0.03695764]]\n",
      "Предсказанный выход после обучения:\n",
      "[0.47208287 0.17617597 0.62324994]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "Sequential = tf.keras.models.Sequential\n",
    "Dense = tf.keras.layers.Dense\n",
    "\n",
    "# Установка скорости обучения\n",
    "learning_rate = 0.3\n",
    "\n",
    "# Входной вектор\n",
    "input_vector = np.array([0.4, -0.7, 1.3])\n",
    "\n",
    "# Эталонный выход\n",
    "target_output = np.array([0.3, -0.5, 0.8])\n",
    "\n",
    "# Инициализация весов случайными значениями из интервала [-0.3, 0.3]\n",
    "weights_input_hidden = np.random.uniform(-0.3, 0.3, (3, 4))   # Веса между входным и скрытым слоями\n",
    "weights_hidden_output = np.random.uniform(-0.3, 0.3, (4, 3))  # Веса между скрытым и выходным слоями\n",
    "\n",
    "# Функция активации: логистическая сигмоида\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Функция активации: логистическая сигмоида.\n",
    "    \n",
    "    Параметры:\n",
    "    x (numpy.ndarray): Входные данные для функции активации.\n",
    "    \n",
    "    Возвращает:\n",
    "    numpy.ndarray: Выходные данные после применения сигмоиды.\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Производная функции активации\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"\n",
    "    Производная функции активации сигмоиды.\n",
    "    \n",
    "    Параметры:\n",
    "    x (numpy.ndarray): Выходные данные функции активации.\n",
    "    \n",
    "    Возвращает:\n",
    "    numpy.ndarray: Производная функции активации.\n",
    "    \"\"\"\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Обучение сети\n",
    "for epoch in range(25):  # Количество эпох\n",
    "    # Прямое распространение\n",
    "    hidden_layer_input = np.dot(input_vector, weights_input_hidden)\n",
    "    hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "    output_layer_input = np.dot(hidden_layer_output, weights_hidden_output)\n",
    "    output_layer_output = sigmoid(output_layer_input)\n",
    "\n",
    "    # Ошибка\n",
    "    error = target_output - output_layer_output\n",
    "\n",
    "    # Обратное распространение\n",
    "    d_predicted_output = error * output_layer_output * (1 - output_layer_output)\n",
    "    error_hidden_layer = d_predicted_output.dot(weights_hidden_output.T)\n",
    "    d_hidden_layer = error_hidden_layer * hidden_layer_output * (\n",
    "        1 - hidden_layer_output)\n",
    "\n",
    "    # Обновление весов\n",
    "    weights_hidden_output += hidden_layer_output.reshape(-1, 1).dot(\n",
    "        d_predicted_output.reshape(1, -1)) * learning_rate\n",
    "    weights_input_hidden += input_vector.reshape(-1, 1).dot(\n",
    "        d_hidden_layer.reshape(1, -1)) * learning_rate\n",
    "\n",
    "# Результаты после обучения\n",
    "print(\"Обученные веса между входным и скрытым слоями:\")\n",
    "print(weights_input_hidden)\n",
    "print(\"Обученные веса между скрытым и выходным слоями:\")\n",
    "print(weights_hidden_output)\n",
    "\n",
    "# Проверка предсказания\n",
    "print(\"Предсказанный выход после обучения:\")\n",
    "print(output_layer_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Контрольные вопросы:\n",
    "**1. Понятие нейронной сети, архитектуры**\n",
    "\n",
    "Нейронная сеть — это вычислительная модель, вдохновленная структурой и функциями биологических нейронных сетей. Она состоит из узлов (нейронов), которые связаны между собой и могут обрабатывать информацию.\n",
    "\n",
    "Архитектура нейронной сети определяется количеством слоев (входной, скрытые и выходной) и количеством нейронов в каждом слое.\n",
    "\n",
    "**2. Обучение нейронной сети**\n",
    "\n",
    "Обучение нейронной сети — это процесс, в ходе которого сеть настраивает свои параметры (веса) на основе обучающей выборки. Это позволяет сети минимизировать ошибку предсказания и улучшать свою производительность на новых данных.\n",
    "\n",
    "**3. Основные определения: скорость обучения, эпоха, нейрон, обучающая выборка, тестовая выборка, вариационная выборка, функция активации.**\n",
    "\n",
    "- Скорость обучения — это параметр, который определяет, насколько сильно обновляются веса нейронной сети на каждом шаге обучения. Высокая скорость может привести к нестабильности, а низкая — к медленному обучению.\n",
    "- Эпоха — это один полный проход через всю обучающую выборку. Обычно обучение включает множество эпох.\n",
    "- Нейрон — это базовый элемент нейронной сети, который принимает входные данные, применяет к ним веса и функцию активации, чтобы выдать выходное значение.\n",
    "- Обучающая выборка — это набор данных, используемый для обучения модели.\n",
    "- Тестовая выборка — это набор данных, используемый для оценки производительности модели после обучения.\n",
    "- Вариационная выборка — это набор данных, который может использоваться для настройки гиперпараметров модели.\n",
    "- Функция активации — это функция, которая определяет выход нейрона на основе его входных данных. Она помогает сети обучаться сложным зависимостям.\n",
    "\n",
    "**4. Алгоритм обратного распространения ошибки**\n",
    "\n",
    "Алгоритм обратного распространения ошибки (backpropagation) — это метод, используемый для обучения нейронных сетей. Он включает в себя два этапа: прямое распространение, где входные данные проходят через сеть и генерируют выход, и обратное распространение, где вычисляется градиент ошибки и обновляются веса сети. Этот процесс повторяется до тех пор, пока ошибка не станет приемлемой.\n",
    "\n",
    "**5. Типы функций активации**\n",
    "\n",
    "Существует несколько типов функций активации, включая:\n",
    "- Сигмоидная функция — преобразует входные данные в диапазон от 0 до 1.\n",
    "- Гиперболический тангенс — преобразует входные данные в диапазон от -1 до 1.\n",
    "- ReLU (Rectified Linear Unit) — возвращает 0 для отрицательных входов и сам вход для положительных, что помогает избежать проблемы затухающего градиента.\n",
    "\n",
    "**6. Алгоритмы обучения**\n",
    "\n",
    "Существует несколько алгоритмов обучения нейронных сетей, включая:\n",
    "- Градиентный спуск — основной метод, который минимизирует функцию потерь, обновляя веса в направлении отрицательного градиента.\n",
    "- Стохастический градиентный спуск (SGD) — вариант градиентного спуска, который обновляет веса на основе одного примера из обучающей выборки.\n",
    "- Адаптивные методы (например, Adam, RMSprop) — алгоритмы, которые адаптируют скорость обучения для каждого веса на основе их градиентов."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
